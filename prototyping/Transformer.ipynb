{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Needed for training and evaluation\n",
    "from losses import *\n",
    "from RandomMatrixDataSet import get_sample\n",
    "from RandomMatrixDataSet import RandomMatrixDataSet\n",
    "from RandomMatrixDataSet import SingularvalueMatrix\n",
    "from RandomMatrixDataSet import EigenMatrix\n",
    "from train import train_on_batch\n",
    "from train import run_training\n",
    "from evaluation import *\n",
    "from plotting import plot_loss_logs\n",
    "from plotting import error_histogram\n",
    "from plotting import plot_mean_identity_approx\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from typing import Tuple\n",
    "from typing import Optional\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformer import TransformerModel\n",
    "from transformer import PositionalEncoding\n",
    "from transformer import device\n",
    "\n",
    "#Seed and looks\n",
    "torch.random.seed = 1234\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94130e",
   "metadata": {},
   "source": [
    "### Here I generate a src-tgt pair to validate the pipe-line (or a test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0824ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(matrix_parameters):\n",
    "    M = get_sample(matrix_parameters)\n",
    "    M.compute_labels()\n",
    "    # print(f'==[ M.X:\\n{M.X}')\n",
    "    # data = torch.flatten(M.X, start_dim = 2)\n",
    "    # target = torch.flatten(M.Y, start_dim = 2)\n",
    "    # return data, target\n",
    "    # return M.X, target\n",
    "    return M.X, M.Y\n",
    "\n",
    "# This is the loss Ive used in previous training\n",
    "mseloss = nn.MSELoss()\n",
    "\n",
    "def MSE_loss(out, tgt, src):\n",
    "   \n",
    "    return mseloss(out, tgt)\n",
    "\n",
    "def relative_inv_MSE_loss(out, tgt, src):\n",
    "    #Normalize with batch square mean?\n",
    "    # id_approx = torch.matmul(predicted,x)\n",
    "    # print(f'==[ predicted: {predicted.shape}')\n",
    "    # print(f'==[ x.shape: {src.shape}')\n",
    "    \n",
    "    id = torch.eye(src.shape[-1])[None,:,:].repeat(src.shape[0],1,1)\n",
    "    # print(f'==[ id:\\n{id}')\n",
    "    \n",
    "    # print(f'==[ out.squeeze(1): {out.squeeze(1)}') \n",
    "    # print(f'==[ src.squeeze(1): {src.squeeze(1)}') \n",
    "    \n",
    "    id_approx = torch.bmm(out.squeeze(1), src.squeeze(1))\n",
    "    # print(f'==[ id_approx: {id_approx}')\n",
    "    \n",
    "    return (id_approx - id).square().mean()\n",
    "\n",
    "def train(model: nn.Module, lossfun: Callable) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    #num_batches = len(train_data) // bptt\n",
    "    for i in range(iterations):\n",
    "        # Matrices of random size between 3 and 7\n",
    "        train_params['d'] = random.randint(3,7)\n",
    "        src, tgt = get_batch(train_params)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt)\n",
    "        loss = lossfun(out, tgt, src)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            #ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.5f}')\n",
    "            total_loss = 0\n",
    "            # print(f'==[ out:\\n{out}')\n",
    "            start_time = time.time()\n",
    "           \n",
    "def evaluate(model: nn.Module, eval_parameters) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        src, tgt = get_batch(eval_parameters)\n",
    "        output = model(src, tgt)\n",
    "        output = output.squeeze()\n",
    "        \n",
    "        total_loss += N * criterion(output_flat, tgt.reshape(-1,n_tokens)).item()\n",
    "    return total_loss / (eval_parameters[\"N\"] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edae233",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2348d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix parameters\n",
    "m_size = 3 # Matrix size\n",
    "k_size = (2,2) # Kernel size\n",
    "max_seq_length = 128 # Maximum sequence length\n",
    "\n",
    "# Model parameters\n",
    "d_hid = 64 # Dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_heads = 1  # number of heads in nn.MultiheadAttention\n",
    "n_layers = 1  # Number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.0  # dropout probability\n",
    "           \n",
    "model = TransformerModel(d_hid, n_heads, n_layers, dropout, max_seq_length, k_size).to(device)\n",
    "\n",
    "# Training and evaluation parameters\n",
    "N = 32 # Batch size\n",
    "train_params = {\n",
    "      \"N\": N,\n",
    "      \"d\": m_size\n",
    "}\n",
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "iterations = 1000\n",
    "best_model = None\n",
    "eval_params = {\n",
    "      \"N\": 100,\n",
    "      \"d\": m_size\n",
    "}\n",
    "\n",
    "lr = 1e-3 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# lossfun = MSE_loss\n",
    "lossfun = relative_inv_MSE_loss\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Debug\n",
    "# ------------------------------------------------------------------------------\n",
    "src, tgt = get_batch(train_params)\n",
    "# print(f'==[ Source:\\n{src}')\n",
    "# print(f'==[ Source shape:\\n{src.shape}')\n",
    "# print(f'==[ Target:\\n{tgt}')\n",
    "# print(f'==[ Target shape:\\n{tgt.shape}')\n",
    "\n",
    "input_ones = torch.ones((1,*src.shape[1:]), dtype=src.dtype)\n",
    "divisor = F.fold(F.unfold(input_ones,kernel_size=k_size),kernel_size=k_size, output_size=src.shape[-2:])\n",
    "# print(f'==[ divisor: {divisor}')\n",
    "\n",
    "unfolded = F.unfold(tgt, kernel_size=k_size)\n",
    "# print(f'==[ Unfolded:\\n{unfolded}')\n",
    "# print(f'==[ Unfolded shape:\\n{unfolded.shape}')\n",
    "\n",
    "folded = F.fold(unfolded, kernel_size=k_size, output_size=src.shape[-2:]) / divisor\n",
    "# print(f'==[ Folded:\\n{folded}')\n",
    "# print(f'==[ Folded shape:\\n{folded.shape}')\n",
    "\n",
    "if not torch.equal(folded, tgt):\n",
    "      raise ValueError(\"Invalid unfold/fold operation!\")\n",
    "\n",
    "pos_encoder= PositionalEncoding(math.prod(k_size))\n",
    "src_pos_encoded = pos_encoder.forward(unfolded.permute(0,2,1))\n",
    "# print(f'==[ src_pos_encoded:\\n{src_pos_encoded}')\n",
    "# print(f'==[ src_pos_encoded shape:\\n{src_pos_encoded.shape}')\n",
    "\n",
    "out = model(src, tgt)\n",
    "# print(f'==[ out: {out}')\n",
    "# print(f'==[ out shape: {out.shape}')\n",
    "\n",
    "loss = lossfun(out, tgt, src)\n",
    "# ------------------------------------------------------------------------------\n",
    "# /Debug\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, lossfun)\n",
    "   #  val_loss = evaluate(model, eval_parameters)\n",
    "   #  print(f'==[ val_loss: {val_loss}')\n",
    "   #  val_ppl = math.exp(val_loss)\n",
    "   #  elapsed = time.time() - epoch_start_time\n",
    "   #  print('-' * 89)\n",
    "   #  print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "   #       f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "   #  print('-' * 89)\n",
    "\n",
    "   #  if val_loss < best_val_loss:\n",
    "   #     best_val_loss = val_loss\n",
    "   #     best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(src,tgt)\n",
    "\n",
    "# print(out) #Just goes to some (or two) fixed values \n",
    "#print(t)\n",
    "print(f'==[ out: {out}')\n",
    "mia = torch.bmm(out.squeeze(1), src.squeeze(1))[0]\n",
    "print(f'==[ mia: {mia}')\n",
    "mia = mia.detach().numpy()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd47f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "img = ax.imshow(mia, cmap='spring')\n",
    "for i in range(m_size):\n",
    "    for j in range(m_size):\n",
    "        t = ax.text(j, i, round(mia[i, j], 4),\n",
    "                        ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "    # Create colorbar\n",
    "cbar = ax.figure.colorbar(img, ax=ax)\n",
    "ax.set_xticks(np.arange(0, m_size, 1) + 0.5)\n",
    "ax.set_yticks(np.arange(0, m_size, 1) + 0.5)\n",
    "ax.set_title('Mean $f(X)X$', fontsize=18)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_yticklabels('')\n",
    "ax.set_title(\"A less underwhelming Transformer inverter\", fontsize = 16)\n",
    "ax.grid(color=\"w\", linestyle='-', linewidth=3)\n",
    "ax.tick_params(bottom=False, left=False);\n",
    "plt.savefig(\"transverter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d3e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
