{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from ConvNet import ConvNet\n",
    "from MLP import MLP\n",
    "from UNET import UNET\n",
    "from losses import *\n",
    "from siren import Siren\n",
    "from RandomMatrixDataSet import *\n",
    "torch.random.seed = 1234\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing matrix generation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 \n",
    "E = EigenMatrix(N,mu = 1,sigma = 10)\n",
    "S = SingularvalueMatrix(N,cond = 10)\n",
    "M = RandomMatrixDataSet(N)\n",
    "M.from_eigenvalues(sigma = 1, diagonal = True)\n",
    "M.get_cond()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = 3 #Matrix dimension\n",
    "cond = 10\n",
    "# Define model(s) w. hyperparameters\n",
    "hidden_layers = 5 #Number of hidden layers\n",
    "hidden_features = 32\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "\n",
    "#Initate models\n",
    "reluMLP = MLP(d,hidden_layers, hidden_features)\n",
    "CNN = ConvNet(hidden_layers,filters,kernel_size)\n",
    "UNet = UNET(1,1)\n",
    "SIREN = Siren(d,hidden_features, hidden_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize network \n",
    "model = reluMLP\n",
    "\n",
    "# Training parameters\n",
    "loss_fcn = inv_MSE\n",
    "\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "k = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation data with a fixed condition number\n",
    "X_eval = RandomMatrixDataSet(100, d = d)\n",
    "#X_eval.from_condition_number(10)\n",
    "X_eval.from_eigenvalues(sigma = 10, diagonal = True)\n",
    "x_eval = X_eval.X\n",
    "y_eval = X_eval.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Training\n",
    "def train_on_batch(batch, model,loss_fcn, optimizer, scheduler = None):\n",
    "    pred = model(batch.X)\n",
    "    if loss_fcn == inv_MSE or loss_fcn == inv_RMSE or loss_fcn == inv_frobenius:\n",
    "        loss = loss_fcn(pred,batch.X)\n",
    "    else:\n",
    "        loss = loss_fcn(pred,batch.Y)\n",
    "\n",
    "    #Zero the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Perform a step in LR scheduler to update LR\n",
    "    if scheduler:\n",
    "        scheduler.step(loss.item())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_training(k,model):\n",
    "    # When a new network is created we init empty training logs\n",
    "    loss_log = []\n",
    "    eval_loss_log = []\n",
    "    weighted_average_log = []\n",
    "    weighted_average = deque([], maxlen=100)\n",
    "\n",
    "\n",
    "    # And store the best results\n",
    "    best_loss = np.inf\n",
    "    best_model_state_dict = model.state_dict()\n",
    "\n",
    "    for i in range(k):\n",
    "        #Sample random matrices with same singular values small condition number\n",
    "        batch = RandomMatrixDataSet(batch_size, d = d)\n",
    "        #batch.from_condition_number(cond)\n",
    "        #Sample random matrices with \n",
    "        batch.from_eigenvalues(sigma = 10, diagonal = True)\n",
    "        \n",
    "        loss = train_on_batch(batch,model,loss_fcn, optimizer) #/mean_cond  #LOOK IF THIS SHOULD BE REDEFINED\n",
    "\n",
    "\n",
    "        # We store the model if it has the lowest fitness\n",
    "        # (this is to avoid losing good results during a run that goes wild)\n",
    "        if loss < best_loss:\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            best_loss = loss\n",
    "            #print('New Best: ', loss.item())\n",
    "\n",
    "        # Update the loss trend indicators\n",
    "        weighted_average.append(loss.item())\n",
    "\n",
    "        # Update the logs\n",
    "        weighted_average_log.append(np.mean(weighted_average))\n",
    "        loss_log.append(loss.item())\n",
    "        if i % 100 == 0:\n",
    "            pred_on_eval = model(x_eval)\n",
    "            eval_loss = loss_fcn(pred_on_eval,x_eval)\n",
    "            eval_loss_log.append(eval_loss)\n",
    "\n",
    "        # Print every i iterations\n",
    "        if i % 1000 == 0:\n",
    "            wa_out = np.mean(weighted_average)\n",
    "            print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t eval_loss={eval_loss:.3e}\\t\")\n",
    "\n",
    "  \n",
    "    #def save_results():\n",
    "    #\n",
    "    return model,loss_log,weighted_average_log, eval_loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model,loss_log,weighted_average_log,eval_loss_log = run_training(k,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preliminary results from training. For more eval, see Model_Evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect if the model overfitted/how it trained on the data.\n",
    "pred = trained_model(x_eval)\n",
    "\n",
    "fig = plt.figure()\n",
    "#Visualize how well the network output serves as an inverse\n",
    "ax = fig.add_subplot(121)\n",
    "mean_identity_approx = torch.matmul(pred,x_eval).mean(0)[0].detach().numpy()\n",
    "img = ax.imshow(mean_identity_approx, cmap = 'summer')\n",
    "for i in range(d):\n",
    "        for j in range(d):\n",
    "            t= ax.text(j, i, round(mean_identity_approx[i, j],4),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize = 16)\n",
    "# Create colorbar\n",
    "cbar = ax.figure.colorbar(img, ax=ax)\n",
    "ax.set_xticks(np.arange(0,3,1)+0.5)\n",
    "ax.set_yticks(np.arange(0,3,1)+0.5)\n",
    "ax.set_title('Mean $f(X)X$', fontsize = 18)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_yticklabels('')\n",
    "ax.grid(color=\"w\", linestyle='-', linewidth=3)\n",
    "ax.tick_params( bottom=False, left=False)\n",
    "\n",
    "# Plot loss logs\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(loss_log, label = \"Training loss\")\n",
    "ax.plot(weighted_average_log, label = \"Weighted average\")\n",
    "ax.plot(np.linspace(0,k,len(eval_loss_log)), eval_loss_log, label= \"Evaluation loss\")\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_title('Loss logs', fontsize = 18)\n",
    "ax.legend(fontsize = 14)\n",
    "fig.suptitle('Training results', fontsize=20)\n",
    "ax.set_xlabel('Iteration', fontsize=18)\n",
    "ylabel = ax.set_ylabel('Loss', fontsize=16)\n",
    "plt.savefig(\"training_results_diag.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
