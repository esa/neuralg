{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from ConvNet import ConvNet\n",
    "from reluMLP import MLP\n",
    "from UNET import UNET\n",
    "torch.random.seed =1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(N,eigen_values = None, mu = 1,sigma = 0.2,d = 3,similar = True):\n",
    "    # Generate N invertible matrices of dimension d\n",
    "    # Returns a tensor as default.  \n",
    "    # Generate eigenvalues of matrices of reasonable size, close to eachother\n",
    "    # Allows specifying the eigenvalues of the matrices by passing \n",
    "    if eigen_values != None:\n",
    "        x = eig.repeat(N,1)\n",
    "        x = x[:,:,None]\n",
    "    else:\n",
    "        x = mu*torch.ones((N,d,1)) + sigma*torch.rand(N,d,1) \n",
    "    \n",
    "    #Create diagonal matrices \n",
    "    diag  = torch.eye(x.shape[1])*x[:,None] \n",
    "\n",
    "    #Transformation matrix for similarity transform \n",
    "    if not similar:\n",
    "        #Creates matrices with different basis. \n",
    "        M = torch.rand(N,1,d,d)\n",
    "        X = torch.matmul(torch.matmul(M,diag),torch.linalg.inv(M))\n",
    "    #Do similarity transform with same basis\n",
    "    else: \n",
    "        M = torch.rand(d,d)\n",
    "        X = torch.matmul(torch.matmul(M,diag),torch.linalg.inv(M))\n",
    "    \n",
    "    #X = torch.Tensor(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# Initiate model(s) w. hyperparameters\n",
    "d = 3 #Matrix dimension\n",
    "L = 5 #Number of hidden layers\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "\n",
    "MLP = reluMLP(d,L)\n",
    "CNN = ConvNet(L,filters,kernel_size)\n",
    "UNet = UNET(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "test_loss_log = []\n",
    "weighted_average_log = []\n",
    "weighted_average = deque([], maxlen=100)\n",
    "\n",
    "\n",
    "# And store the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data to (over)fit the model to just one (or more) similar matrices\n",
    "x_train = generate_matrix(1000, similar = True)\n",
    "x_eval = generate_matrix(100,similar = True)\n",
    "\n",
    "#print(x_train[0])\n",
    "#print(torch.linalg.inv(x_train)[0])\n",
    "#model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "id = torch.eye(3,3)\n",
    "model = CNN\n",
    "for i in range(10000):\n",
    "    # create random matrices\n",
    "    x = x_train #For over-fitting \n",
    "    #x = generate_matrix(batch_size) #for \"full training set\"\n",
    "    pred = model(x)\n",
    "    mean_cond = torch.linalg.cond(x).mean() \n",
    "    # Compute loss between network and numerical solution by how well it serves as an inverse\n",
    "    \n",
    "    # Normalize with condition number, dont now if it makes sense but difficult to get it to converge/very unstable otherwise\n",
    "    loss = (torch.matmul(pred,x) - id).square().mean()#/mean_cond  #LOOK IF THIS SHOULD BE REDEFINED\n",
    "    #loss = (torch.matmul(pred,x) - id).square().sum((2,3)).mean()/mean_cond #Frobenius norm\n",
    "    #loss = (pred -torch.linalg.inv(x)).square().mean()\n",
    "    \n",
    "    # We store the model if it has the lowest fitness\n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        #print('New Best: ', loss.item())\n",
    "\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "    \n",
    "\n",
    "    # Print every i iterations\n",
    "    if i % 100 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t\")\n",
    "        test_loss_log.append((torch.matmul(model(x_eval),x_eval) - id).square().mean().item())\n",
    "\n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preliminary results from training. For more eval, see Model_Evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect if the model overfitted/how it trained on the data.\n",
    "pred = model(x_train)\n",
    "#print(torch.inverse(X_data) - pred ) \n",
    "print(pred.shape)\n",
    "print(\"Approximation of identity matrix:\\n \", torch.matmul(pred,x_train)) #Should be the identity matrix.\n",
    "print(\"loss = \" , (torch.matmul(pred,x) - id).square().mean())\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "plt.plot(loss_log, label = \"Training loss\")\n",
    "plt.plot(np.arange(1,10000,100), test_loss_log, label= \"Test loss\")\n",
    "\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.legend()\n",
    "fig.suptitle('Loss log', fontsize=18)\n",
    "plt.xlabel('Iteration', fontsize=18)\n",
    "ylab = plt.ylabel('Loss', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets try to see how it generalizes, although still generated in the same manner \n",
    "x_test = generate_matrix(100, similar = True)\n",
    "inv_pred = model(x_test)\n",
    "torch.matmul(inv_pred,x_test)\n",
    "test_loss =(torch.matmul(inv_pred,x_test) - id).square().sum((2,3)).detach().numpy()\n",
    "\n",
    "print(\"Approximation of identity matrix:\\n \",torch.matmul(inv_pred,x_test)[1])\n",
    "print(\"loss = \", (torch.matmul(inv_pred,x_test) - id).square().mean().item())\n",
    "\n",
    "print(\"MSE = \", (torch.linalg.inv(x_test) - inv_pred).square().mean().item())\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "plt.scatter(torch.linalg.cond(x_test),test_loss)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "fig.suptitle('Preliminary evaluation', fontsize=18)\n",
    "plt.xlabel('Condition number', fontsize=18)\n",
    "ylab = plt.ylabel('Test loss', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"CNN_mu1sigma02_similar.pt\")\n",
    "#torch.save(model.state_dict(),\"MLP_mu1sigma02_similar.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
