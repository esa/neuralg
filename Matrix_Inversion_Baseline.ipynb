{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(N,mu = 1,sigma = 0.2,d = 3,similar = True):\n",
    "    # Generate N invertible matrices of dimension d\n",
    "    # Returns a tensor as default.  \n",
    "    # Generate eigenvalues of matrices of reasonable size, close to eachother\n",
    "    \n",
    "    #Fix seed? \n",
    "    #random.seed(1234) \n",
    "    x = mu*np.ones((N,d,1), dtype = np.float64) + sigma*np.random.randn(N,d,1) \n",
    "    \n",
    "    #Create diagonal matrices \n",
    "    diag  = np.eye(x.shape[1])*x[:,np.newaxis] \n",
    "\n",
    "    #Transformation matrix for similarity transform \n",
    "    if not similar:\n",
    "        #Creates matrices with different basis. \n",
    "        M = np.random.randn(N,1,d,d)\n",
    "        X = np.matmul(np.matmul(M,diag),np.linalg.inv(M))\n",
    "    #Do similarity transform with same basis\n",
    "    else: \n",
    "        M = np.random.randn(d,d)\n",
    "        X = np.matmul(np.matmul(M,diag),np.linalg.inv(M))\n",
    "    \n",
    "    X = torch.Tensor(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on matrix generation: \n",
    "Maybe instead one should work with deterministic eigenvalues to begin with to have complete control, and only introduce diversity in the data set via the similarity transform. \n",
    "Alternatively, and perhaps especially for similar matrices, one can use the same fixed transformation matrix when generating. Feels like could make online sampling easier. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        self.unflatten = nn.Unflatten(-1,(3,3))\n",
    "        self.relu_stack = nn.Sequential(\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_stack(x)\n",
    "        x = self.unflatten(x)\n",
    "        return x\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple MLP\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(64)\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3,padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3,padding=\"same\")\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3,padding=\"same\")\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3,padding=\"same\")\n",
    "        self.conv5 = nn.Conv2d(64, 64, 3,padding=\"same\")\n",
    "        self.conv6 = nn.Conv2d(64, 1, 3,padding=\"same\")\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        return x\n",
    "\n",
    "#model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNET Architecture, not really working for 3x3. \n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNET,self).__init__()\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 3,1)\n",
    "        #self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        #self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "        #self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        #self.upconv2 = self.expand_block(64, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32, out_channels, 3, 1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        #conv2 = self.conv2(conv1)\n",
    "        #conv3 = self.conv3(conv2)\n",
    "\n",
    "        #upconv3 = self.upconv3(conv3)\n",
    "        #upconv2 = self.upconv2(conv2)\n",
    "        #upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        #upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "        upconv1 = self.upconv1(conv1)\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "                                 )\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2, padding=1, output_padding=1)\n",
    "                            )\n",
    "        return expand\n",
    "#model = UNET(1,1)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 100\n",
    "lr = 1e-1\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "weighted_average = deque([], maxlen=100)\n",
    "\n",
    "\n",
    "# And store the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training data to (over)fit the model to just one (or more) similar matrices\n",
    "x_train = generate_matrix(1000, similar = True)\n",
    "x_zero = torch.zeros(1,1,3,3)\n",
    "print(x_train[0])\n",
    "print(torch.linalg.inv(x_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Training Loop\n",
    "id = torch.eye(3,3)\n",
    "\n",
    "for i in range(10000):\n",
    "    # create random matrices\n",
    "    x = x_train #For over-fitting \n",
    "    #x = generate_matrix(batch_size) #for \"full training set\"\n",
    "    pred = model(x)\n",
    "    mean_cond = torch.linalg.cond(x).mean() \n",
    "    # Compute loss between network and numerical solution by how well it serves as an inverse\n",
    "    \n",
    "    # Normalize with condition number, dont now if it makes sense but difficult to get it to converge/very unstable otherwise\n",
    "    loss = (torch.matmul(pred,x) - id).square().mean()#/mean_cond  #LOOK IF THIS SHOULD BE REDEFINED\n",
    "    #loss = (torch.matmul(pred,x) - id).square().sum((2,3)).mean()/mean_cond #Frobenius norm\n",
    "    #loss = (pred -torch.linalg.inv(x)).square().mean()\n",
    "    \n",
    "    # We store the model if it has the lowest fitness\n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        #print('New Best: ', loss.item())\n",
    "\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    # Print every i iterations\n",
    "    if i % 100 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t\")\n",
    "\n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preliminary results from training. For more eval, see Model_Evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect if the model overfitted/how it trained on the data.\n",
    "pred = model(x_train)\n",
    "#print(torch.inverse(X_data) - pred ) \n",
    "print(pred.shape)\n",
    "print(\"Approximation of identity matrix:\\n \", torch.matmul(pred,x_train)) #Should be the identity matrix.\n",
    "print(\"loss = \" , (torch.matmul(pred,x) - id).square().mean())\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "plt.plot(loss_log)\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "fig.suptitle('Loss log', fontsize=18)\n",
    "plt.xlabel('Iteration', fontsize=18)\n",
    "ylab = plt.ylabel('Training loss', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets try to see how it generalizes, although still generated in the same manner \n",
    "x_test = generate_matrix(100, similar = True)\n",
    "inv_pred = model(x_test)\n",
    "torch.matmul(inv_pred,x_test)\n",
    "test_loss =(torch.matmul(inv_pred,x_test) - id).square().sum((2,3)).detach().numpy()\n",
    "\n",
    "print(\"Approximation of identity matrix:\\n \",torch.matmul(inv_pred,x_test)[1])\n",
    "print(\"loss = \", (torch.matmul(inv_pred,x_test) - id).square().mean().item())\n",
    "\n",
    "print(\"MSE = \", (torch.linalg.inv(x_test) - inv_pred).square().mean().item())\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "plt.scatter(torch.linalg.cond(x_test),test_loss)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "fig.suptitle('Preliminary evaluation', fontsize=18)\n",
    "plt.xlabel('Condition number', fontsize=18)\n",
    "ylab = plt.ylabel('Test loss', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"CNN_mu1sigma02_similar.pt\")\n",
    "#torch.save(model.state_dict(),\"MLP_mu1sigma02_similar.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
