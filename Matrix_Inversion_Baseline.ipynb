{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Models\n",
    "from ConvNet import ConvNet\n",
    "from MLP import MLP\n",
    "from UNET import UNET\n",
    "from losses import *\n",
    "from siren import Siren\n",
    "\n",
    "#Needed for training and evaluation\n",
    "from RandomMatrixDataSet import RandomMatrixDataSet,SingularvalueMatrix, EigenMatrix\n",
    "from train import train_on_batch, run_training\n",
    "from evaluation import model_evaluation\n",
    "\n",
    "#Seed and looks\n",
    "torch.random.seed = 1234\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustration of random matrix generation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2 \n",
    "E = EigenMatrix(N,mu = 1,sigma = 10)\n",
    "S = SingularvalueMatrix(N,cond = 10)\n",
    "M = RandomMatrixDataSet(N)\n",
    "M.from_eigenvalues(mu = 2, sigma = 0.2, diagonal = True)\n",
    "M.compute_determinant()\n",
    "M.det\n",
    "M.compute_cond()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = 3 #Matrix dimension\n",
    "cond = 10\n",
    "\n",
    "# Define model(s) hyperparameters\n",
    "hidden_layers = 5 \n",
    "hidden_features = 32\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "\n",
    "#Initate some example models\n",
    "reluMLP = MLP(d,hidden_layers, hidden_features)\n",
    "CNN = ConvNet(hidden_layers,filters,kernel_size)\n",
    "UNet = UNET(1,1)\n",
    "SIREN = Siren(d,hidden_features, hidden_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize network, for example here simple MLP\n",
    "model = reluMLP\n",
    "\n",
    "# Training parameters\n",
    "loss_fcn = inv_MSE\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "k = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Characterize matrices for training. For example, diagonal matrices with randomized eigenvalues\n",
    "matrix_parameters = {\"N\":batch_size,\n",
    "                   \"d\": 3,\n",
    "                  #\"cond\": 10,\n",
    "                  \"mu\": 1,\n",
    "                  \"sigma\":5,\n",
    "                  \"diagonal\": True,\n",
    "                  \"similar\": None}\n",
    "#Run training\n",
    "trained_model,loss_log,weighted_average_log,eval_loss_log,x_eval = run_training(k,model,loss_fcn,optimizer, matrix_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preliminary results from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect if the model overfitted/how it trained on the data.\n",
    "pred = trained_model(x_eval)\n",
    "\n",
    "fig = plt.figure()\n",
    "#Visualize how well the network output serves as an inverse\n",
    "ax = fig.add_subplot(121)\n",
    "mean_identity_approx = torch.matmul(pred,x_eval).mean(0)[0].detach().numpy()\n",
    "img = ax.imshow(mean_identity_approx, cmap = 'summer')\n",
    "for i in range(d):\n",
    "        for j in range(d):\n",
    "            t= ax.text(j, i, round(mean_identity_approx[i, j],4),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize = 16)\n",
    "# Create colorbar\n",
    "cbar = ax.figure.colorbar(img, ax=ax)\n",
    "ax.set_xticks(np.arange(0,3,1)+0.5)\n",
    "ax.set_yticks(np.arange(0,3,1)+0.5)\n",
    "ax.set_title('Mean $f(X)X$', fontsize = 18)\n",
    "ax.set_xticklabels('')\n",
    "ax.set_yticklabels('')\n",
    "ax.grid(color=\"w\", linestyle='-', linewidth=3)\n",
    "ax.tick_params( bottom=False, left=False)\n",
    "\n",
    "# Plot loss logs\n",
    "ax = fig.add_subplot(122)\n",
    "ax.plot(loss_log, label = \"Training loss\")\n",
    "ax.plot(weighted_average_log, label = \"Weighted average\")\n",
    "ax.plot(np.linspace(0,k,len(eval_loss_log)), eval_loss_log, label= \"Evaluation loss\")\n",
    "ax.set_yscale('log')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_title('Loss logs', fontsize = 18)\n",
    "ax.legend(fontsize = 14)\n",
    "fig.suptitle('Training results', fontsize=20)\n",
    "ax.set_xlabel('Iteration', fontsize=18)\n",
    "ylabel = ax.set_ylabel('Loss', fontsize=16)\n",
    "#plt.savefig(\"training_results_diag.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of evaluation module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parameters = {\"N\":1000,\n",
    "                   \"d\": 3,\n",
    "                  #\"cond\": 10,\n",
    "                  \"mu\": 1,\n",
    "                  \"sigma\":0.3,\n",
    "                  \"diagonal\": True,\n",
    "                  \"similar\": True}\n",
    "m = model_evaluation(trained_model,test_parameters)\n",
    "fig = plt.figure()\n",
    "\n",
    "#Visualize how well the network output serves as an inverse\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(np.log10(m[\"errors\"]), bins = 40, edgecolor='black', alpha=0.65)\n",
    "ax.axvline(np.log10(m[\"mean_error\"]), color='r', linestyle='dashed', linewidth=1, label = \"Mean test error\")\n",
    "ax.set_xlabel(\"$log_{10}(error)$\", fontsize = 16)\n",
    "ax.set_ylabel(\"Frequency\", fontsize = 16)\n",
    "ax.legend(fontsize = 14)\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(np.log10(m[\"mean_error\"])*0.95, max_ylim*0.8, 'Mean: {:.3f}'.format(m[\"mean_error\"]), fontsize = 14)\n",
    "ax.set_title(\"Evaluation results\", fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
