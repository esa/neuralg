{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=2, end_dim=-1)\n",
      "  (unflatten): Unflatten(dim=-1, unflattened_size=(3, 3))\n",
      "  (relu_stack): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=9, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Sigmoid()\n",
      "    (3): ReLU()\n",
      "    (4): Sigmoid()\n",
      "    (5): ReLU()\n",
      "    (6): Sigmoid()\n",
      "    (7): ReLU()\n",
      "    (8): Sigmoid()\n",
      "    (9): ReLU()\n",
      "    (10): Sigmoid()\n",
      "    (11): Linear(in_features=9, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[-0.4223, -0.3669,  0.5406],\n          [ 0.3375, -0.0517,  0.3179],\n          [ 0.8510,  0.1866, -0.0635]]]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple MLP\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        #self.flatten = nn.Flatten(start_dim=2)\n",
    "        #self.unflatten = nn.Unflatten(1,(3,3))\n",
    "        self.relu_stack = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ReLU(),\n",
    "            nn.ReLU(),\n",
    "            nn.ReLU(),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        x = self.relu_stack(x)\n",
    "        #x = self.unflatten(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "weighted_average = deque([], maxlen=100)\n",
    "\n",
    "\n",
    "# And store the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best:  0.3415718376636505\n",
      "It=0\t loss=3.416e-01\t  weighted_average=3.416e-01\t\n",
      "New Best:  0.33730068802833557\n",
      "New Best:  0.31803134083747864\n",
      "New Best:  0.3142189681529999\n",
      "New Best:  0.3023819327354431\n",
      "New Best:  0.29980066418647766\n",
      "New Best:  0.2942497432231903\n",
      "New Best:  0.293241024017334\n",
      "New Best:  0.2829401195049286\n",
      "New Best:  0.28268536925315857\n",
      "New Best:  0.2685202956199646\n",
      "New Best:  0.262570858001709\n",
      "New Best:  0.26035648584365845\n",
      "New Best:  0.25847604870796204\n",
      "New Best:  0.2578469514846802\n",
      "New Best:  0.254860520362854\n",
      "New Best:  0.2531417906284332\n",
      "New Best:  0.2486710548400879\n",
      "New Best:  0.24754352867603302\n",
      "New Best:  0.24669446051120758\n",
      "It=100\t loss=2.532e-01\t  weighted_average=2.713e-01\t\n",
      "New Best:  0.24440936744213104\n",
      "New Best:  0.24343113601207733\n",
      "New Best:  0.2430737167596817\n",
      "New Best:  0.24216103553771973\n",
      "New Best:  0.24138802289962769\n",
      "New Best:  0.2374332696199417\n",
      "New Best:  0.23700568079948425\n",
      "It=200\t loss=2.531e-01\t  weighted_average=2.476e-01\t\n",
      "New Best:  0.2336723357439041\n",
      "New Best:  0.2324162870645523\n",
      "It=300\t loss=2.384e-01\t  weighted_average=2.412e-01\t\n",
      "New Best:  0.23236083984375\n",
      "New Best:  0.2321919947862625\n",
      "It=400\t loss=2.406e-01\t  weighted_average=2.380e-01\t\n",
      "New Best:  0.2318095564842224\n",
      "It=500\t loss=2.364e-01\t  weighted_average=2.358e-01\t\n",
      "New Best:  0.23165485262870789\n",
      "New Best:  0.2311049848794937\n",
      "It=600\t loss=2.342e-01\t  weighted_average=2.349e-01\t\n",
      "It=700\t loss=2.340e-01\t  weighted_average=2.344e-01\t\n",
      "New Best:  0.23075439035892487\n",
      "It=800\t loss=2.338e-01\t  weighted_average=2.335e-01\t\n",
      "It=900\t loss=2.331e-01\t  weighted_average=2.337e-01\t\n",
      "It=1000\t loss=2.325e-01\t  weighted_average=2.336e-01\t\n",
      "It=1100\t loss=2.356e-01\t  weighted_average=2.335e-01\t\n",
      "It=1200\t loss=2.328e-01\t  weighted_average=2.334e-01\t\n",
      "It=1300\t loss=2.329e-01\t  weighted_average=2.334e-01\t\n",
      "It=1400\t loss=2.333e-01\t  weighted_average=2.333e-01\t\n",
      "It=1500\t loss=2.326e-01\t  weighted_average=2.335e-01\t\n",
      "It=1600\t loss=2.349e-01\t  weighted_average=2.334e-01\t\n",
      "It=1700\t loss=2.344e-01\t  weighted_average=2.334e-01\t\n",
      "It=1800\t loss=2.340e-01\t  weighted_average=2.334e-01\t\n",
      "It=1900\t loss=2.342e-01\t  weighted_average=2.334e-01\t\n",
      "It=2000\t loss=2.331e-01\t  weighted_average=2.334e-01\t\n",
      "It=2100\t loss=2.335e-01\t  weighted_average=2.333e-01\t\n",
      "It=2200\t loss=2.334e-01\t  weighted_average=2.333e-01\t\n",
      "It=2300\t loss=2.336e-01\t  weighted_average=2.334e-01\t\n",
      "It=2400\t loss=2.327e-01\t  weighted_average=2.333e-01\t\n",
      "It=2500\t loss=2.329e-01\t  weighted_average=2.333e-01\t\n",
      "It=2600\t loss=2.326e-01\t  weighted_average=2.334e-01\t\n",
      "It=2700\t loss=2.327e-01\t  weighted_average=2.334e-01\t\n",
      "It=2800\t loss=2.322e-01\t  weighted_average=2.335e-01\t\n",
      "It=2900\t loss=2.346e-01\t  weighted_average=2.333e-01\t\n",
      "It=3000\t loss=2.327e-01\t  weighted_average=2.332e-01\t\n",
      "It=3100\t loss=2.328e-01\t  weighted_average=2.334e-01\t\n",
      "It=3200\t loss=2.330e-01\t  weighted_average=2.333e-01\t\n",
      "It=3300\t loss=2.331e-01\t  weighted_average=2.334e-01\t\n",
      "It=3400\t loss=2.336e-01\t  weighted_average=2.332e-01\t\n",
      "It=3500\t loss=2.339e-01\t  weighted_average=2.333e-01\t\n",
      "It=3600\t loss=2.332e-01\t  weighted_average=2.333e-01\t\n",
      "It=3700\t loss=2.322e-01\t  weighted_average=2.333e-01\t\n",
      "It=3800\t loss=2.337e-01\t  weighted_average=2.333e-01\t\n",
      "It=3900\t loss=2.335e-01\t  weighted_average=2.333e-01\t\n",
      "It=4000\t loss=2.344e-01\t  weighted_average=2.332e-01\t\n",
      "It=4100\t loss=2.336e-01\t  weighted_average=2.334e-01\t\n",
      "It=4200\t loss=2.338e-01\t  weighted_average=2.335e-01\t\n",
      "It=4300\t loss=2.340e-01\t  weighted_average=2.335e-01\t\n",
      "It=4400\t loss=2.323e-01\t  weighted_average=2.334e-01\t\n",
      "It=4500\t loss=2.328e-01\t  weighted_average=2.333e-01\t\n",
      "It=4600\t loss=2.341e-01\t  weighted_average=2.333e-01\t\n",
      "New Best:  0.23072150349617004\n",
      "It=4700\t loss=2.339e-01\t  weighted_average=2.333e-01\t\n",
      "New Best:  0.23070703446865082\n",
      "It=4800\t loss=2.344e-01\t  weighted_average=2.334e-01\t\n",
      "It=4900\t loss=2.329e-01\t  weighted_average=2.332e-01\t\n",
      "It=5000\t loss=2.326e-01\t  weighted_average=2.333e-01\t\n",
      "It=5100\t loss=2.341e-01\t  weighted_average=2.334e-01\t\n",
      "It=5200\t loss=2.324e-01\t  weighted_average=2.333e-01\t\n",
      "It=5300\t loss=2.333e-01\t  weighted_average=2.335e-01\t\n",
      "It=5400\t loss=2.323e-01\t  weighted_average=2.334e-01\t\n",
      "It=5500\t loss=2.337e-01\t  weighted_average=2.333e-01\t\n",
      "It=5600\t loss=2.332e-01\t  weighted_average=2.333e-01\t\n",
      "It=5700\t loss=2.334e-01\t  weighted_average=2.335e-01\t\n",
      "It=5800\t loss=2.331e-01\t  weighted_average=2.333e-01\t\n",
      "It=5900\t loss=2.321e-01\t  weighted_average=2.335e-01\t\n",
      "It=6000\t loss=2.325e-01\t  weighted_average=2.332e-01\t\n",
      "It=6100\t loss=2.328e-01\t  weighted_average=2.333e-01\t\n",
      "It=6200\t loss=2.335e-01\t  weighted_average=2.334e-01\t\n",
      "It=6300\t loss=2.332e-01\t  weighted_average=2.333e-01\t\n",
      "It=6400\t loss=2.336e-01\t  weighted_average=2.331e-01\t\n",
      "It=6500\t loss=2.329e-01\t  weighted_average=2.333e-01\t\n",
      "It=6600\t loss=2.331e-01\t  weighted_average=2.334e-01\t\n",
      "It=6700\t loss=2.338e-01\t  weighted_average=2.333e-01\t\n",
      "It=6800\t loss=2.327e-01\t  weighted_average=2.333e-01\t\n",
      "It=6900\t loss=2.335e-01\t  weighted_average=2.332e-01\t\n",
      "It=7000\t loss=2.324e-01\t  weighted_average=2.334e-01\t\n",
      "It=7100\t loss=2.340e-01\t  weighted_average=2.333e-01\t\n",
      "It=7200\t loss=2.323e-01\t  weighted_average=2.333e-01\t\n",
      "It=7300\t loss=2.332e-01\t  weighted_average=2.334e-01\t\n",
      "It=7400\t loss=2.339e-01\t  weighted_average=2.333e-01\t\n",
      "It=7500\t loss=2.341e-01\t  weighted_average=2.332e-01\t\n",
      "It=7600\t loss=2.341e-01\t  weighted_average=2.334e-01\t\n",
      "It=7700\t loss=2.340e-01\t  weighted_average=2.334e-01\t\n",
      "It=7800\t loss=2.339e-01\t  weighted_average=2.334e-01\t\n",
      "It=7900\t loss=2.343e-01\t  weighted_average=2.335e-01\t\n",
      "It=8000\t loss=2.332e-01\t  weighted_average=2.334e-01\t\n",
      "It=8100\t loss=2.343e-01\t  weighted_average=2.333e-01\t\n",
      "It=8200\t loss=2.324e-01\t  weighted_average=2.334e-01\t\n",
      "It=8300\t loss=2.341e-01\t  weighted_average=2.333e-01\t\n",
      "It=8400\t loss=2.349e-01\t  weighted_average=2.335e-01\t\n",
      "It=8500\t loss=2.336e-01\t  weighted_average=2.334e-01\t\n",
      "It=8600\t loss=2.327e-01\t  weighted_average=2.333e-01\t\n",
      "It=8700\t loss=2.333e-01\t  weighted_average=2.335e-01\t\n",
      "It=8800\t loss=2.327e-01\t  weighted_average=2.333e-01\t\n",
      "It=8900\t loss=2.332e-01\t  weighted_average=2.333e-01\t\n",
      "It=9000\t loss=2.331e-01\t  weighted_average=2.333e-01\t\n",
      "It=9100\t loss=2.327e-01\t  weighted_average=2.333e-01\t\n",
      "It=9200\t loss=2.328e-01\t  weighted_average=2.333e-01\t\n",
      "New Best:  0.23039884865283966\n",
      "It=9300\t loss=2.329e-01\t  weighted_average=2.333e-01\t\n",
      "It=9400\t loss=2.337e-01\t  weighted_average=2.332e-01\t\n",
      "New Best:  0.23013946413993835\n",
      "It=9500\t loss=2.301e-01\t  weighted_average=2.333e-01\t\n",
      "It=9600\t loss=2.343e-01\t  weighted_average=2.332e-01\t\n",
      "It=9700\t loss=2.331e-01\t  weighted_average=2.335e-01\t\n",
      "It=9800\t loss=2.346e-01\t  weighted_average=2.333e-01\t\n",
      "It=9900\t loss=2.336e-01\t  weighted_average=2.333e-01\t\n"
     ]
    }
   ],
   "source": [
    "### Training Loop\n",
    "for i in range(10000):\n",
    "\n",
    "    # create random matrices\n",
    "    x = torch.rand((batch_size,1,3,3))\n",
    "\n",
    "    pred = model(x)\n",
    "    print(pred.shape())\n",
    "    id = torch.eye(3,3)\n",
    "    inv_x = torch.inverse(x)\n",
    "\n",
    "    # Compute loss between network and numerical solution\n",
    "    loss = (pred*x - id).square().mean()\n",
    "\n",
    "    # We store the model if it has the lowest fitness\n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        print('New Best: ', loss.item())\n",
    "\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    # Print every i iterations\n",
    "    if i % 100 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t\")\n",
    "\n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1449, 0.1482, 0.3835],\n",
      "          [0.8750, 0.3568, 0.0071],\n",
      "          [0.6605, 0.5584, 0.1736]]]])\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example\n",
    "m = torch.rand((1,1,3,3))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 0.6938,  2.2544, -1.6248],\n          [-1.7613, -2.7300,  4.0029],\n          [ 3.0261,  0.2033, -0.9330]]]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.inverse(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1996, 0.1979, 0.2019],\n",
      "          [0.1992, 0.1996, 0.2007],\n",
      "          [0.1992, 0.1987, 0.1985]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model(m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}