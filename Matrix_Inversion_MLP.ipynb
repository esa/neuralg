{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(N,d = 3, tensor = True, similar = True):\n",
    "    # Generate N invertible matrices of dimension d with small condition number. \n",
    "    # Returns a tensor as default.  \n",
    "    \n",
    "    #Generate eigenvalues of matrices of reasonable size, close to eachother\n",
    "    x = 2*np.ones((N,d,1), dtype = np.float64) - 0.5*np.random.randn(N,d,1) \n",
    "    \n",
    "    #Create diagonal matrices \n",
    "    diag  = np.eye(x.shape[1])*x[:,np.newaxis] \n",
    "    \n",
    "    #Transformation matrix for similarity transform \n",
    "    if not similar:\n",
    "        #Creates matrices with different basis.\n",
    "        M = np.random.randn(N,d,d)\n",
    "        X = np.dot(np.dot(M,diag),np.linalg.inv(M))\n",
    "    #Do similarity transform with same basis\n",
    "    M = np.random.randn(d,d)\n",
    "    X = np.matmul(np.matmul(M,diag),np.linalg.inv(M))\n",
    "    if tensor:\n",
    "        X = torch.Tensor(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=2)\n",
    "        self.unflatten = nn.Unflatten(-1,(3,3))\n",
    "        self.relu_stack = nn.Sequential(\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(9,9),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu_stack(x)\n",
    "        x = self.unflatten(x)\n",
    "        return x\n",
    "model = MLP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (upconv1): Sequential(\n",
      "    (0): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(1, 1, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNET,self).__init__()\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 3,1)\n",
    "        #self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        #self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "        #self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        #self.upconv2 = self.expand_block(64, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32, out_channels, 3, 1)\n",
    "    def forward(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        #conv2 = self.conv2(conv1)\n",
    "        #conv3 = self.conv3(conv2)\n",
    "\n",
    "        #upconv3 = self.upconv3(conv3)\n",
    "        #upconv2 = self.upconv2(conv2)\n",
    "        #upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        #upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "        upconv1 = self.upconv1(conv1)\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2, padding=1, output_padding=1)\n",
    "                            )\n",
    "        return expand\n",
    "model = UNET(1,1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 100\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "\n",
    "# When a new network is created we init empty training logs\n",
    "loss_log = []\n",
    "weighted_average_log = []\n",
    "weighted_average = deque([], maxlen=100)\n",
    "\n",
    "\n",
    "# And store the best results\n",
    "best_loss = np.inf\n",
    "best_model_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0995, -0.0995, -0.0995],\n",
      "          [-0.1553,  0.1004, -0.2278],\n",
      "          [-0.0988,  0.1621, -0.0978]]]],\n",
      "       grad_fn=<SlowConvTranspose2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Training data to (over)fit the model \n",
    "X_data = generate_matrix(1)\n",
    "x = X_data\n",
    "#x\n",
    "#model(x)\n",
    "print(model(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It=0\t loss=3.523e-01\t  weighted_average=3.523e-01\t\n",
      "It=1000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=2000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=3000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=4000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=5000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=6000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=7000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=8000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n",
      "It=9000\t loss=1.905e-02\t  weighted_average=1.905e-02\t\n"
     ]
    }
   ],
   "source": [
    "### Training Loop\n",
    "id = torch.eye(3,3)\n",
    "for i in range(10000):\n",
    "    # create random matrices\n",
    "    #x = generate_matrix(batch_size, similar = False)\n",
    "    pred = model(x)\n",
    "\n",
    "    # Compute loss between network and numerical solution by how well it serves as an inverse\n",
    "    loss = (torch.matmul(pred,x) - id).square().mean()\n",
    "\n",
    "    # We store the model if it has the lowest fitness\n",
    "    # (this is to avoid losing good results during a run that goes wild)\n",
    "    if loss < best_loss:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_loss = loss\n",
    "        #print('New Best: ', loss.item())\n",
    "\n",
    "    # Update the loss trend indicators\n",
    "    weighted_average.append(loss.item())\n",
    "\n",
    "    # Update the logs\n",
    "    weighted_average_log.append(np.mean(weighted_average))\n",
    "    loss_log.append(loss.item())\n",
    "\n",
    "    # Print every i iterations\n",
    "    if i % 1000 == 0:\n",
    "        wa_out = np.mean(weighted_average)\n",
    "        print(f\"It={i}\\t loss={loss.item():.3e}\\t  weighted_average={wa_out:.3e}\\t\")\n",
    "\n",
    "    # Zeroes the gradient (necessary because of things)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of identity matrix:\n",
      "  tensor([[[[ 1.0168,  0.1736,  0.1448],\n",
      "          [ 0.0377,  1.0086,  0.0271],\n",
      "          [-0.2741, -0.0629,  0.8030]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "loss =  tensor(0.0191, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Test to see if the model overfitted \n",
    "pred = model(X_data)\n",
    "#print(torch.inverse(X_data) - pred ) \n",
    "#print(pred)\n",
    "print(\"Approximation of identity matrix:\\n \", torch.matmul(pred,X_data)) #Should be the identity matrix.\n",
    "print(\"loss = \" , (torch.matmul(pred,x) - id).square().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation of identity matrix:\n",
      "  tensor([[[[ 1.4844,  0.1928,  1.8354],\n",
      "          [ 0.0063,  1.1366, -0.5787],\n",
      "          [-0.0851, -0.1782,  1.0838]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2463, -0.1116,  0.7568],\n",
      "          [ 0.0864,  1.2187,  1.1738],\n",
      "          [-0.0723, -0.0809,  0.9094]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "loss =  0.3410756289958954\n",
      "MSE =  0.06486474722623825\n"
     ]
    }
   ],
   "source": [
    "# Lets try to see how it generalizes, although still generated in the same manner \n",
    "x_test = generate_matrix(2, similar = True)\n",
    "inv_pred = model(x_test)\n",
    "torch.matmul(inv_pred,x_test)\n",
    "\n",
    "print(\"Approximation of identity matrix:\\n \",torch.matmul(inv_pred,x_test))\n",
    "print(\"loss = \", (torch.matmul(inv_pred,x_test) - id).square().mean().item())\n",
    "\n",
    "print(\"MSE = \", (torch.linalg.inv(x_test) - inv_pred).square().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# What if we change the structure even more? \n",
    "x_test = torch.Tensor(np.random.rand(1,1,3,3))\n",
    "print(x_test)\n",
    "inv_pred = model(x_test)\n",
    "torch.matmul(inv_pred,x_test)\n",
    "\n",
    "print(\"Approximation of identity matrix:\\n \",torch.matmul(inv_pred,x_test))\n",
    "print(\"loss = \", (torch.matmul(inv_pred,x_test) - id).square().mean())\n",
    "\n",
    "print(\"MSE = \", (torch.linalg.inv(x_test) - inv_pred).square().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (pythonProject1)",
   "language": "python",
   "name": "pycharm-8a7ee7e4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
