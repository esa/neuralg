{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for loading, training and using customized `neuralg` models\n",
    "   But first, imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append main folder\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Supporting packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotmap import DotMap\n",
    "from copy import deepcopy\n",
    "\n",
    "# Imports from module\n",
    "from neuralg.io.get_model import get_model\n",
    "from neuralg.io.save_model import save_model\n",
    "from neuralg.io.delete_model import delete_model\n",
    "from neuralg.training.train_model import train_model\n",
    "from neuralg.utils.set_up_torch import set_up_torch\n",
    "from neuralg.ops.svd import svd\n",
    "from neuralg.evaluation.evaluate_model import evaluate_model\n",
    "from neuralg.evaluation.compute_accuracy import compute_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load and re-train existing models\n",
    "This particular tutorial is devoted to the `svd` operation, i.e. finding singular values of matrices. However, the equivalent procedures are applicable to all supported operations in the module. \n",
    "\n",
    "The default models in `neuralg` designated to approximate singular values are trained on random square matrices with elements uniformly distributed on [-10,10]. Depending on the user application, the performance on matrices with different properties might not be immediately satisfactory. \n",
    "\n",
    "Let's assume that in your application, it is more likely that the matrix elements are realisations of standard gaussian random variables. To this end, the `neuralg` built-in training loop supports fine-tuning the existing models by re-training on such matrices. This tutorial serves as an explanation how to do so. Alternatively, if you already have a generated data set or a more exotic matrix distribution you can simply load the model of choice to fine-tune in your own training loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model \n",
    "To load a model, you need to specify the target operation and the matrix size. Note that with this call, the matrix size must be available for the target operation. See [README](linktoreadme) for detailed specification on supported sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = \"svd\"\n",
    "matrix_dimension = 20\n",
    "svd_model = get_model(operation, matrix_dimension) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you want to define a new model from scratch - in that case, you can instantiate a new model by calling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_svd_model = get_model(operation,100, new = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small pre-training evaluation\n",
    "As a baseline reference, we look at how the out-of-the-neuralg-box model performs on the target matrices. For this purpose, we consider a predicted collection of singular values $\\hat{\\sigma}$ to be a correct solution to the problem if it approximates the ground truth $\\sigma$ to a given tolerance $\\tau$. Specifically, we check that $||\\sigma-\\hat{\\sigma}||_{1} \\leq \\tau||\\sigma||_{1}$ is satisfied. The ground truths are computed with `torch.svdvals()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.1 # We set tolerance to 10% \n",
    "eval_set_size = 10000 # And evaluate on this many matrices\n",
    "\n",
    "# The way matrices are generated in the module, it requires a dict or a DotMap passing the batch parameters, e.g. :\n",
    "test_matrix_parameters = DotMap({\"N\":eval_set_size, \n",
    "                            \"operation\": operation, \n",
    "                            \"d\": matrix_dimension, \n",
    "                            \"normal\": True, # Default is uniform entries\n",
    "                            \"sigma\": 1,     # Standard deviation of normal elements. Default is 10/sqrt(3) \n",
    "                            })\n",
    "svd_model.eval()\n",
    "errors = evaluate_model(svd_model, test_matrix_parameters) # Compute the L1 errors on the evaluation set\n",
    "accuracy_before_training = compute_accuracy(tol,errors) # And assess the accuracy\n",
    "\n",
    "print(f\"With a tolerance of {tol:.0%}, evaluated model achieved accuracy {accuracy_before_training:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with the in-place training loop\n",
    "The model does not generalize particularly well to the target matrices. We will try to improve the accuracy by re-training the model on the target matrices by accessing the in-place training loop.\n",
    "#### Specifying data generation and training parameters\n",
    "The way the training loop is implemented, it requires passing training specifications as dicts or dotmaps alongside the model. Specifically, we need to define the properties of the matrix batches to train on and run parameters such as how many iterations the optimisation algorithm should run for and initial learning rate. \n",
    "\n",
    "Default optimizer is the Adam algorithm and an exponential learning rate schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "training_matrix_parameters = deepcopy(test_matrix_parameters) # We can just copy the target matrix settings,\n",
    "training_matrix_parameters.N = batch_size      # But change the number of matrices\n",
    "training_run_parameters = DotMap({ \"epoch\": 1, # Number of epochs\n",
    "                           \"iterations\": 50, # Batches per epoch\n",
    "                           \"lr\": 3e-5} )       # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training \n",
    "The function `train_model()` will return a dotmap with the trained model, alongside the loss logs from the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_up_torch(\"float32\", torch_enable_cuda= True) # Enable training on the GPU, if available\n",
    "\n",
    "svd_model.train()\n",
    "training_results = train_model(svd_model,training_matrix_parameters, training_run_parameters) # Run training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-training evaluation\n",
    "Now we hope to see some performace improvement for the target matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_svd_model = training_results.model # Get the trained model \n",
    "trained_svd_model.eval()\n",
    "# Same evaluation procedure as pre-training\n",
    "errors = evaluate_model(trained_svd_model, test_matrix_parameters) # Compute the L1 errors on the evaluation set\n",
    "accuracy_after_training = compute_accuracy(tol,errors) # Assess accuracy\n",
    "print(f\"With a tolerance of {tol:.0%}, model achieved accuracy {accuracy_after_training:.3} on the evaluation set after training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and using a custom model \n",
    "\n",
    "#### Save custom model\n",
    "If the training outcome was satisfactory, you can save the model by naming it and call `save_model()`. This will save the best model state dict from training to a directory in the user project folder called custom_models. If the directory does not exist it will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"svd_standard_normal\" # Preferably an informative name\n",
    "save_model(trained_svd_model, model_name) # Save model state dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use it in the module! \n",
    "To be able to use the customized model for future singular value approximations, pass the new model name as an optional argument to `svd()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either by defining a lambda function,\n",
    "svd_standard_normal = lambda x: svd(x,custom_model_name=model_name)\n",
    "\n",
    "m = torch.randn(matrix_dimension,matrix_dimension) # Sample test matrix\n",
    "\n",
    "singular_value_predictions = svd_standard_normal(m) # Predict using the defined custom operation\n",
    "\n",
    "# Or equivalently calling svd() directly with the additional argument \n",
    "singular_value_predictions = svd(m,custom_model_name=model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "### Plot singular value sample\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(np.arange(1,matrix_dimension+1), singular_value_predictions.detach().numpy(),s = 200, color='pink', marker='o', label = \"neuralg\")\n",
    "ax.scatter(np.arange(1,matrix_dimension+1), torch.linalg.svdvals(m).detach().numpy(), s = 200, color='purple', marker='x', label = \"torch\")\n",
    "ax.set_title(\"Sample singular value approximation\", fontsize = 18)\n",
    "ax.legend(fontsize = 16, loc = \"best\")\n",
    "ax.set_xticks(np.arange(0, matrix_dimension, 1) + 1)\n",
    "ax.set_xlabel(\" Singular value \", fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting models\n",
    "If a model is now longer of use or the module is getting large, you might want to delete a model by calling`delete_model()`. This will clear the folder custom_models from the requested model state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_model(model_name) # Delete the customized saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more customizing: Adding your own model ideas\n",
    "#### Defining a custom model class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagConvNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_layers, filters, kernel_size):\n",
    "        super(DiagConvNet, self).__init__()\n",
    "        self.net = []\n",
    "        self.net.append(torch.nn.Conv2d(1, filters, kernel_size, padding=\"same\"))\n",
    "        self.net.append(torch.nn.BatchNorm2d(filters))\n",
    "        self.net.append(torch.nn.ReLU())\n",
    "        for i in range(hidden_layers - 1):\n",
    "            self.net.append(\n",
    "                torch.nn.Conv2d(filters, filters, kernel_size, padding=\"same\")\n",
    "            )\n",
    "            self.net.append(torch.nn.BatchNorm2d(filters))\n",
    "            self.net.append(torch.nn.ReLU())\n",
    "\n",
    "        self.net.append(torch.nn.Conv2d(filters, 1, kernel_size, padding=\"same\"))\n",
    "        self.net = torch.nn.Sequential(*self.net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = torch.diagonal(out, 0, 2, 3)\n",
    "        return out\n",
    "\n",
    "model = DiagConvNet(5,32,3)\n",
    "save_model(model,\"prototype_model\", custom_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a customized model type in the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.randn(1,1,matrix_dimension,matrix_dimension) # Dealing with a convolutional network, we add a batch and channel dimension\n",
    "singular_value_predictions = svd(matrix, custom_model_name=\"prototype_model\", custom_model_class = True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33b13c0f5861550def7c5fd31dfce4700e1b96089b8cc03c6412b21d90f85adf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('freshpy3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
