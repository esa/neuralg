{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small test demo of the neuralg eig function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralg \n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx # used to generate graphs\n",
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module allows for setting precision and potentially employed on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralg.set_precision(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral graph theory application\n",
    "\n",
    "In spectral graph theory, the graph properties are studied in light of the eigendecompositions of their associated matrices. \n",
    "\n",
    "In the cell below, we generate $N$ random undirected graphs each with $n$ number of nodes, drawing a possble edge with probability $p$, and save the corresponding Adjacency and Laplacian matrices incl. their eigenvalue spectras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "number_of_nodes = np.arange(5,11)\n",
    "undirected_graphs = DotMap()\n",
    "for n in number_of_nodes:\n",
    "    for i in range(N):\n",
    "        g = networkx.erdos_renyi_graph(n=n, p= 0.5)\n",
    "        adjacency_matrix = torch.tensor(networkx.to_numpy_array(g))[None,:]\n",
    "        laplacian_matrix = torch.tensor(networkx.laplacian_matrix(g).toarray(), dtype = torch.float64 )[None,:]\n",
    "        if i == 0: \n",
    "            A = adjacency_matrix\n",
    "            L = laplacian_matrix\n",
    "            \n",
    "        else:   \n",
    "            A = torch.cat((A,adjacency_matrix),0)\n",
    "            L = torch.cat((L,laplacian_matrix),0)\n",
    "    undirected_graphs[n].adjacency = A\n",
    "    undirected_graphs[n].adjacency.eigvals = neuralg.eig(A)\n",
    "    undirected_graphs[n].laplacian = L\n",
    "    undirected_graphs[n].laplacian.eigvals = neuralg.eig(L)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After using the module, we can clear the loaded models to free the allocated memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralg.clear_loaded_models()\n",
    "assert neuralg.neuralg_ModelHandler.loaded_models == {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A machine learning task using the neuralg module\n",
    "\n",
    "Lets say we want a neural network that approximates the total number of length $k$ cycles starting from any node. In exact arithmetic, this quantity is given by \n",
    "\n",
    "$$trace(A^k),$$\n",
    "\n",
    "where $A$ is the adjacency matrix of the graph, but we do not want to perform the $k$ matrix multiplications. From the properties of the trace operator, we can relate this quantity to  the spectrum of $A$ such that  \n",
    "\n",
    "$$ \\# \\text{cycles of length } k = \\sum_{i} \\lambda_i(A)^k $$ \n",
    "\n",
    "To this end, we can use the eig function from the neuralg module to approximate the ground truths in the supervised learning of predicting this quantity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a simple convolutional net for the regression task. A convolutional block is followed by two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CycleCNN(nn.Module):\n",
    "    def __init__(self, n_graph_nodes, conv_layers, filters, kernel_size):\n",
    "        super(CycleCNN, self).__init__()\n",
    "        self.net = []\n",
    "        self.n_graph_nodes = n_graph_nodes\n",
    "        self.net.append(nn.Conv2d(1,filters,kernel_size, padding = \"same\"))\n",
    "        self.net.append(nn.BatchNorm2d(filters))\n",
    "        self.net.append(nn.ReLU())\n",
    "        for i in range(conv_layers-1):\n",
    "            self.net.append(nn.Conv2d(filters,filters,kernel_size, padding = \"same\"))\n",
    "            self.net.append(nn.BatchNorm2d(filters))\n",
    "            self.net.append(nn.ReLU())\n",
    "        \n",
    "        self.net.append(nn.Conv2d(filters,1,kernel_size, padding = \"same\"))\n",
    "        self.net.append(nn.Flatten())\n",
    "        self.net.append(DenseLayer(n_graph_nodes**2,n_graph_nodes))\n",
    "        self.net.append(DenseLayer(n_graph_nodes,1, is_last = True))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_last = False):\n",
    "        super().__init__()\n",
    "        self.is_last = is_last\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.is_last: \n",
    "            return self.linear(input)\n",
    "        else:\n",
    "            return F.relu(self.linear(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define graph generation properties and target cycle length and instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data parameters \n",
    "n_graph_nodes = 5 #This\n",
    "k = 5 #Cycle length\n",
    "p = 0.5\n",
    "\n",
    "#Instantiate model\n",
    "model = CycleCNN(n_graph_nodes,conv_layers = 3,filters = 32,kernel_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "#Training parameters\n",
    "iterations = 10000\n",
    "batch_size = 32\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 3e-4 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model.train(); # turn on train mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_batch(batch_size,n_graph_nodes):\n",
    "    \"\"\"Stupid generator of random graph adjacency matrices\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Sife of batch to be generated\n",
    "\n",
    "    Returns:\n",
    "        tensor: Tensor of shape [batchsize,1,n_graph_nodes,n_graph_nodes] \n",
    "    \"\"\"\n",
    "    for i in range(batch_size):\n",
    "        g = networkx.erdos_renyi_graph(n=n_graph_nodes, p= 0.5)\n",
    "        adj_matrix = torch.tensor(networkx.to_numpy_array(g))[None,:]\n",
    "        if i == 0: \n",
    "            A = adj_matrix\n",
    "        else:   \n",
    "            A = torch.cat((A,adj_matrix),0)\n",
    "        \n",
    "    return A[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 1000\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        #Sample a batch\n",
    "        A = get_adjacency_batch(batch_size, n_graph_nodes= n_graph_nodes)\n",
    "    \n",
    "\n",
    "        #Predict # of k-cycles with model\n",
    "        output = model(A)\n",
    "\n",
    "        #Use module to compute ground truth\n",
    "        target_eigvals = neuralg.eig(A)\n",
    "        target = torch.pow(target_eigvals,k).sum(-1)\n",
    "        \n",
    "        loss = criterion(output,target)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "\n",
    "            print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.5f}')\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "best_model = None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some preliminary evaluation from training\n",
    "We can use the previously generated graphs to evaluate the trained model. We also include the eigenvalue ground truths using the torch module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = undirected_graphs[n_graph_nodes].adjacency[:,None,:] #A dummy index for the channel dimension is added\n",
    "\n",
    "eval_pred = torch.round(model(eval_set)) #Since path length should be an integer, we round the predictions\n",
    "neuralg_targets = torch.round(torch.pow(neuralg.eig(eval_set),k).sum(-1)) #Compute ground truth with neuralg module\n",
    "torch_targets = torch.round(torch.pow(torch.real(torch.linalg.eigvals(eval_set)),k).sum(-1)) #Compute ground truth with torch built-int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralg.evaluation.compute_accuracy import compute_accuracy\n",
    "from neuralg.training.losses import relative_L1_evaluation_error\n",
    "\n",
    "neuralg_eval_errors= relative_L1_evaluation_error(eval_pred,neuralg_targets)\n",
    "torch_eval_errors = relative_L1_evaluation_error(eval_pred,torch_targets) \n",
    "\n",
    "tol = 0.1\n",
    "\n",
    "neuralg_acc = compute_accuracy(tol,neuralg_eval_errors).item()\n",
    "torch_acc = compute_accuracy(tol,torch_eval_errors).item()\n",
    "\n",
    "print(\"Tolerance set to {:.0%}\".format(tol))\n",
    "print(\"Test accuracy with neuralg ground truths = {}\".format(neuralg_acc))\n",
    "print(\"Test accuracy with torch ground truths = {}\" .format(torch_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "939eb026ed6100385ed54bc0e5d104be5eda2ea460a65f3624aaec41a577b131"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('Project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
