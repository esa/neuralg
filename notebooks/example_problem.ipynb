{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07:29:16\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mInitialized neuralg for cpu\u001b[0m\n",
      "\u001b[32m07:29:16\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mSetting Torch's default tensor type to Float32 (CUDA not initialized).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import neuralg \n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx\n",
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07:29:18\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mSetting Torch's default tensor type to Float64 (CUDA not initialized).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "neuralg.set_precision(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral graph theory application\n",
    "\n",
    "Generate $N$ random undirected graphs each with $n$ number of nodes, drawing a possble edge with probability $p$,\n",
    "and compute the eigenvalues of the corresponding Adjacency and Laplacian matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'networkx.utils.decorators.argmap'> compilation 8:4: FutureWarning: laplacian_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "number_of_nodes = np.arange(5,11)\n",
    "undirected_graphs = DotMap()\n",
    "for n in number_of_nodes:\n",
    "    for i in range(N):\n",
    "        g = networkx.erdos_renyi_graph(n=n, p= 0.5)\n",
    "        adjacency_matrix = torch.tensor(networkx.to_numpy_array(g))[None,:]\n",
    "        laplacian_matrix = torch.tensor(networkx.laplacian_matrix(g).toarray(), dtype = torch.float64 )[None,:]\n",
    "        if i == 0: \n",
    "            A = adjacency_matrix\n",
    "            L = laplacian_matrix\n",
    "            \n",
    "        else:   \n",
    "            A = torch.cat((A,adjacency_matrix),0)\n",
    "            L = torch.cat((L,laplacian_matrix),0)\n",
    "    undirected_graphs[str(n)].adjacency = A\n",
    "    undirected_graphs[str(n)].adjacency.eigvals = neuralg.eig(A)\n",
    "    undirected_graphs[str(n)].laplacian = L\n",
    "    undirected_graphs[str(n)].laplacian.eigvals = neuralg.eig(L)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0081, grad_fn=<SumBackward0>)\n",
      "tensor(45379., grad_fn=<RoundBackward>)\n",
      "tensor(47674.0000)\n"
     ]
    }
   ],
   "source": [
    "print(undirected_graphs[\"5\"].adjacency.eigvals[0].sum())\n",
    "print(torch.round(torch.pow(undirected_graphs[\"5\"].adjacency.eigvals[0],10).sum()))\n",
    "print(torch.pow(torch.real(torch.linalg.eigvals(undirected_graphs[\"5\"].adjacency)[0]),10).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07:31:41\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mClearing loaded models\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "neuralg.clear_loaded_models()\n",
    "assert neuralg.neuralg_ModelHandler.loaded_models == {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can do some inference or training on these graphs based on the eigenvalue distributions\n",
    "-  The spectral gap of the Laplacian of a graph is defined as the absolute difference between the two largest eigenvalues \n",
    "-  The number of cycles of length $k$ in a graph can be related to the sum of the eigenvalues to the power k. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we want a neural network that approximates the total number of length k cycles starting from any node. In exact arithmetic, this quantity is given by \n",
    "\n",
    "$$trace(A^k),$$\n",
    "\n",
    "where $A$ is the adjacency matrix of the graph, but we do not want to perform the k matrix multiplications. From the properties of the trace operator, we can relate this quantity to  the spectrum of $A$ such that  \n",
    "\n",
    "$$ \\# \\text{cycles of length } k = \\sum_{i} \\lambda_i(A)^k $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CycleCNN(nn.Module):\n",
    "    def __init__(self, n_graph_nodes, conv_layers, filters, kernel_size):\n",
    "        super(CycleCNN, self).__init__()\n",
    "        self.net = []\n",
    "        self.n_graph_nodes = n_graph_nodes\n",
    "        self.net.append(nn.Conv2d(1,filters,kernel_size, padding = \"same\"))\n",
    "        self.net.append(nn.BatchNorm2d(filters))\n",
    "        self.net.append(nn.ReLU())\n",
    "        for i in range(conv_layers-1):\n",
    "            self.net.append(nn.Conv2d(filters,filters,kernel_size, padding = \"same\"))\n",
    "            self.net.append(nn.BatchNorm2d(filters))\n",
    "            self.net.append(nn.ReLU())\n",
    "        \n",
    "        self.net.append(nn.Conv2d(filters,1,kernel_size, padding = \"same\"))\n",
    "        self.net.append(nn.Flatten())\n",
    "        self.net.append(DenseLayer(n_graph_nodes**2,n_graph_nodes))\n",
    "        self.net.append(DenseLayer(n_graph_nodes,1, is_last = True))\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, is_last = False):\n",
    "        super().__init__()\n",
    "        self.is_last = is_last\n",
    "        self.in_features = in_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.is_last: \n",
    "            return self.linear(input)\n",
    "        else:\n",
    "            return F.relu(self.linear(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==[ output:\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "#Data parameters (Not using at the moment)\n",
    "n_graph_nodes = 5\n",
    "model = CycleCNN(n_graph_nodes,conv_layers= 3,filters = 32,kernel_size=3)\n",
    "k = 5 #Cycle length\n",
    "p = 0.5\n",
    "#batch_size = 32\n",
    "#Training parameters\n",
    "iterations = 10000\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 3e-4 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "model.train()  # turn on train mode\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# A single step for debugging purposes\n",
    "# ------------------------------------------------------------------------------\n",
    "g = networkx.erdos_renyi_graph(n=n_graph_nodes, p= p)\n",
    "adjacency_matrix = torch.tensor(networkx.to_numpy_array(g))[None,None,:]\n",
    "output = model(adjacency_matrix)\n",
    "\n",
    "print(f'==[ output:\\n{output.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #Sample a batch\n",
    "        A = undirected_graphs.adjacency\n",
    "\n",
    "        #Predict # of k-cycles with model\n",
    "        output = model(A)\n",
    "\n",
    "        #Use module to compute base line \n",
    "        target_eigvals = neuralg.eig(A)\n",
    "\n",
    "        target = torch.round(torch.pow(target_eigvals,k).sum())\n",
    "\n",
    "        \n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            #ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
    "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.5f}')\n",
    "            # print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
    "            #      f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "            #      f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets say we want to approximate the determinant of a matrix "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "939eb026ed6100385ed54bc0e5d104be5eda2ea460a65f3624aaec41a577b131"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('Project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
