{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Demo of ```neuralg``` module\n",
                "\n",
                "## Spectral graph theory application\n",
                "\n",
                "In spectral graph theory, graph properties are studied in light of the eigendecompositions of their associated matrices. For instance, graph characteristics can be inferred from the eigenvalue of the adjacency matrix $A$ of a graph, i.e \n",
                "$$ A_{ij} = \\begin{cases} 1 \\; \\text{if there is an edge from $i$ to $j$} \\\\ 0 \\; \\text{otherwise} \\end{cases}$$\n",
                "A more detailed read can be found e.g. [here](https://en.wikipedia.org/wiki/Spectral_graph_theory).\n",
                "## A machine learning task using the neuralg module\n",
                "To carry out automatic differentation to allow, for example, gradient-based optimization, the eigenvalue approximations must not break the gradient flow. Training a neural network with PyTorch will serve as a demonstration that the `neuralg` module indeed preserves gradients. Lets say we want a neural network that approximates the total number of length $k$ cycles starting from any node in a graph. In exact arithmetic, this quantity is given by \n",
                "\n",
                "$$trace(A^k),$$\n",
                "\n",
                "where $A$ is the adjacency matrix of the graph, but we do not want to perform the $k$ matrix multiplications. From the properties of the trace operator, we can relate this quantity to  the spectrum of $A$ such that  \n",
                "\n",
                "\\begin{equation} \\# \\text{ cycles of length } k = \\sum_{i} \\lambda_i(A)^k \n",
                "\\end{equation}\n",
                "\n",
                "\n",
                "To this end, we can use `neuralg.eigvals()` to approximate the ground truths in the supervised learning of predicting this quantity.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m12:02:23\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mInitialized neuralg for cpu\u001b[0m\n",
                        "\u001b[32m12:02:23\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mSetting Torch's default tensor type to Float32 (CUDA not initialized).\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "import neuralg \n",
                "from neuralg.evaluation.compute_accuracy import compute_accuracy\n",
                "from neuralg.training.losses import relative_L1_evaluation_error\n",
                "import torch\n",
                "import networkx # additional requirement for processing graphs \n",
                "from copy import deepcopy\n",
                "import time\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### `neuralg` allows for setting float precision and can potentially be employed on a GPU, if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[32m12:02:27\u001b[0m|neuralg-\u001b[34mWARNING\u001b[0m| \u001b[33m\u001b[1mError enabling CUDA. cuda.is_available() returned False. CPU will be used.\u001b[0m\n",
                        "\u001b[32m12:02:27\u001b[0m|neuralg-\u001b[34mINFO\u001b[0m| \u001b[1mSetting Torch's default tensor type to Float64 (CUDA not initialized).\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "neuralg.set_up_torch(torch_enable_cuda=True)\n",
                "neuralg.set_precision(\"float64\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define a simple convolutional net for the regression task. A convolutional block is followed by two dense layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn \n",
                "import torch.nn.functional as F\n",
                "\n",
                "class CycleCNN(nn.Module):\n",
                "    def __init__(self, n_graph_nodes, conv_layers, filters, kernel_size):\n",
                "        super(CycleCNN, self).__init__()\n",
                "        self.net = []\n",
                "        self.n_graph_nodes = n_graph_nodes\n",
                "        self.net.append(nn.Conv2d(1,filters,kernel_size, padding = \"same\"))\n",
                "        self.net.append(nn.BatchNorm2d(filters))\n",
                "        self.net.append(nn.ReLU())\n",
                "        for i in range(conv_layers-1):\n",
                "            self.net.append(nn.Conv2d(filters,filters,kernel_size, padding = \"same\"))\n",
                "            self.net.append(nn.BatchNorm2d(filters))\n",
                "            self.net.append(nn.ReLU())\n",
                "        \n",
                "        self.net.append(nn.Conv2d(filters,1,kernel_size, padding = \"same\"))\n",
                "        self.net.append(nn.Flatten())\n",
                "        self.net.append(DenseLayer(n_graph_nodes**2,n_graph_nodes))\n",
                "        self.net.append(DenseLayer(n_graph_nodes,1, is_last = True))\n",
                "        self.net = nn.Sequential(*self.net)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        out = self.net(x)\n",
                "        return out\n",
                "\n",
                "class DenseLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, in_features, out_features, bias=True, is_last = False):\n",
                "        super().__init__()\n",
                "        self.is_last = is_last\n",
                "        self.in_features = in_features\n",
                "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
                "\n",
                "    def forward(self, input):\n",
                "        if self.is_last: \n",
                "            return self.linear(input)\n",
                "        else:\n",
                "            return F.relu(self.linear(input))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Define graph generation properties and target cycle length and necessary training parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Data parameters \n",
                "n_graph_nodes = 5 # Number of nodes in the graph, defining the size of the adjacancy matrices\n",
                "k = 5 # Cycle length of interest\n",
                "p = 0.5 # Probability of two nodes having a connecting edge \n",
                "\n",
                "\n",
                "#Training parameters\n",
                "iterations = 2000 # Iterations per epoch\n",
                "batch_size = 32 # Number of matrices per forward pass\n",
                "epochs = 5\n",
                "criterion = nn.MSELoss() # We use a mean square error loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_adjacency_batch(batch_size,n_graph_nodes, fixed_seed = False):\n",
                "    \"\"\" Simple generator of random graph adjacency matrices.       \n",
                "    Args:\n",
                "        batch_size (int): Size of batch to be generated\n",
                "        n_graph_nodes (int): Corresponds to size of the generaed adjacency matrices. \n",
                "        fixed_seed (bool, optional): If true, the seed is fixed in the sampling. Defaults to False.\n",
                "\n",
                "    Returns:\n",
                "        tensor: batch of adjacency matrices, of shape [batchsize,1,n_graph_nodes,n_graph_nodes] \n",
                "    \"\"\"\n",
                "    if fixed_seed:\n",
                "        torch.manual_seed(1)\n",
                "    for i in range(batch_size):\n",
                "        g = networkx.erdos_renyi_graph(n=n_graph_nodes, p= 0.5)\n",
                "        adj_matrix = torch.tensor(networkx.to_numpy_array(g))[None,:]\n",
                "        if i == 0: \n",
                "            A = adj_matrix\n",
                "        else:   \n",
                "            A = torch.cat((A,adj_matrix))\n",
                "        \n",
                "    return A[:,None,:] # broadcasts to an extra channel dimension "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model: nn.Module, use_neuralg=True) -> None:\n",
                "    model.train()  # turn on train mode\n",
                "    total_loss = 0.\n",
                "    log_interval = 1000\n",
                "    start_time = time.time()\n",
                "\n",
                "    for i in range(iterations):\n",
                "\n",
                "        A = get_adjacency_batch(batch_size, n_graph_nodes=n_graph_nodes) #Sample a batch\n",
                "\n",
                "        output = model(A) #Predict # of k-cycles with model\n",
                "\n",
                "    \n",
                "        if use_neuralg:\n",
                "            target_eigvals = neuralg.eigvals(A, symmetric = True) #Use neuralg module to compute ground truth\n",
                "        else:\n",
                "            target_eigvals = torch.linalg.eigvalsh(A) #Or, use torch built-in \n",
                "        \n",
                "        target = torch.pow(target_eigvals,k).sum(-1) #The target is the quantiti in Eq.(1)\n",
                "        \n",
                "        loss = criterion(output,target) # Compute loss\n",
                "    \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        loss.backward()\n",
                "        \n",
                "        optimizer.step()\n",
                "\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        if i % log_interval == 0 and i > 0:\n",
                "            lr = scheduler.get_last_lr()[0]\n",
                "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
                "            cur_loss = total_loss / log_interval\n",
                "\n",
                "            print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
                "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
                "                  f'loss {cur_loss:5.5f}')\n",
                "\n",
                "            total_loss = 0\n",
                "            start_time = time.time()\n",
                "    return ms_per_batch\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training a network \n",
                "##### For reference, two identically initialized models are trained using either `neuralg.eig()` or `torch.linalg.eigvalsh()` to compute the eigenvalue ground truths. The loss is computed between the model outputs and the target function of the eigenvalues from Eq.(1), i.e. the analytical solution to number of total paths of length $k$. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "| epoch   1 |  1000 batches | lr 0.0003 | ms/batch 14.53 | loss 4937.77547\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32m/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000011?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000011?line=9'>10</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000011?line=10'>11</a>\u001b[0m     ms_per_batch \u001b[39m=\u001b[39m train(model,use_neuralg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000011?line=11'>12</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000011?line=12'>13</a>\u001b[0m trained_models\u001b[39m.\u001b[39mappend([deepcopy(model),ms_per_batch])\n",
                        "\u001b[1;32m/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb Cell 10'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, use_neuralg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000009?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output,target) \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000009?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000009?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000009?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/toveagren/ESA/NLAAP/notebooks/example_problem.ipynb#ch0000009?line=28'>29</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
                        "File \u001b[0;32m~/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
                        "File \u001b[0;32m~/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/toveagren/opt/anaconda3/envs/freshpy3/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "trained_models = []\n",
                "for use_neuralg in [True,False]:\n",
                "    model = CycleCNN(n_graph_nodes, conv_layers = 3,filters = 32,kernel_size=3) #Instantiate model\n",
                "    lr = 3e-4 # learning rate\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
                "    model.train(); # turn on train modeepochs = 1\n",
                "    \n",
                "    for epoch in range(1, epochs + 1):\n",
                "        epoch_start_time = time.time()\n",
                "        ms_per_batch = train(model,use_neuralg)\n",
                "        scheduler.step()\n",
                "    trained_models.append([deepcopy(model),ms_per_batch])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Evaluation from training\n",
                "When evaluating on the test set, we consider a predicted output $\\hat{y}$ to be a correct solution to the problem if it approximates the ground truth $y$ to a given tolerance $\\tau$. Specifically, we check that $|y-\\hat{y}| \\leq \\tau|y|$ is satisfied. For completeness, the test set ground truths are calculated both using `neuralg.eig()` and `torch.eigvalsh()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_size = 10000 # Size of evaluation set \n",
                "eval_set = get_adjacency_batch(eval_size, n_graph_nodes, fixed_seed=True) #Sample a test set\n",
                "\n",
                "#Since path length should be an integer, we round the final predictions with the two trained models\n",
                "neuralg_pred = torch.round(trained_models[0][0](eval_set)) # Model trained with neuralg ground truths\n",
                "torch_pred = torch.round(trained_models[1][0](eval_set)) # Model trained with torch ground truths\n",
                "\n",
                "torch_targets = torch.round(torch.pow(torch.linalg.eigvalsh(eval_set),k).sum(-1)) # We evaluation eigenvalues with torch built-int\n",
                "neuralg_targets = torch.round(torch.pow(neuralg.eig(eval_set),k).sum(-1))\n",
                "\n",
                "#Model trained with neuralg \n",
                "neuralg_to_neuralg_eval_errors= relative_L1_evaluation_error(neuralg_pred,neuralg_targets)\n",
                "neuralg_to_torch_eval_errors= relative_L1_evaluation_error(neuralg_pred,torch_targets)\n",
                "\n",
                "#Model trained with torch\n",
                "torch_to_neuralg_eval_errors = relative_L1_evaluation_error(torch_pred,neuralg_targets) \n",
                "torch_to_torch_eval_errors = relative_L1_evaluation_error(torch_pred,torch_targets) \n",
                "\n",
                "tolerances = [0.1,0.08,0.06,0.05,0.025,0.02,0.01,0.005,0.0001]\n",
                "n_neuralg_acc = []\n",
                "n_torch_acc = []\n",
                "t_neuralg_acc = []\n",
                "t_torch_acc = []\n",
                "for tol in tolerances:\n",
                "    n_neuralg_acc.append(compute_accuracy(tol,neuralg_to_neuralg_eval_errors).item())\n",
                "    n_torch_acc.append(compute_accuracy(tol,neuralg_to_torch_eval_errors).item())\n",
                "    t_neuralg_acc.append(compute_accuracy(tol,torch_to_neuralg_eval_errors).item())\n",
                "    t_torch_acc.append(compute_accuracy(tol,torch_to_torch_eval_errors).item())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(14, 6), dpi=150)\n",
                "fig.patch.set_facecolor(\"white\")\n",
                "ax = fig.add_subplot(1,2,1)\n",
                "ax.set_title(\"neuralg trained, {:.3} ms per batch. \\n Test accuracy versus tolerance, {}x{}\".format(trained_models[0][1],n_graph_nodes, n_graph_nodes), fontsize=18)\n",
                "ax.plot(tolerances,n_neuralg_acc,label=\"neuralg evaluation ground truths\")\n",
                "ax.plot(tolerances,n_torch_acc,label=\"torch evaluation ground truths\")\n",
                "\n",
                "ax.legend(fontsize=14)\n",
                "ax.set_xlabel(\"$\\\\tau $\", fontsize=18)\n",
                "ax.set_ylabel(\"Accuracy\", fontsize=16);\n",
                "\n",
                "ax = fig.add_subplot(1,2,2)\n",
                "ax.set_title(\"torch trained, {:.3} ms per batch.\\n Test accuracy versus tolerance, {}x{}\".format(trained_models[1][1],n_graph_nodes, n_graph_nodes), fontsize=18)\n",
                "ax.plot(tolerances,t_neuralg_acc,label=\"neuralg evaluation ground truths\")\n",
                "ax.plot(tolerances,t_torch_acc,label=\"torch evaluation ground truths\")\n",
                "\n",
                "ax.legend(fontsize=14)\n",
                "ax.set_xlabel(\"$\\\\tau $\", fontsize=18)\n",
                "ax.set_ylabel(\"Accuracy\", fontsize=16);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Conclusion \n",
                "- In evaluation, the model trained with `neuralg` eigenvalues performs well also on the more accurate torch evaluatiom ground truths. \n",
                "- Training with `neuralg` or `torch` modules seem to yield similarly performing models. \n",
                "- When run On CPU there is no synchronization needed and for these small matrices, the computational cost of the numerical methods is very small.  For larger matrices and on the GPU, we expect `neuralg`to have an edge. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### After using the module, we can clear the loaded models to free the allocated memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "neuralg.clear_loaded_models()\n",
                "assert neuralg.neuralg_ModelHandler.loaded_models == {}"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "33b13c0f5861550def7c5fd31dfce4700e1b96089b8cc03c6412b21d90f85adf"
        },
        "kernelspec": {
            "display_name": "PyCharm (pythonProject1)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
