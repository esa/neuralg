{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Demo of ```neuralg``` module\n",
                "\n",
                "## Spectral graph theory application\n",
                "\n",
                "In spectral graph theory, graph properties are studied in light of the eigendecompositions of their associated matrices. For instance, graph characteristics can be inferred from the eigenvalue of the adjacency matrix $A$ of a graph, i.e \n",
                "$$ A_{ij} = \\begin{cases} 1 \\; \\text{if there is an edge from $i$ to $j$} \\\\ 0 \\; \\text{otherwise} \\end{cases}$$\n",
                "A more detailed read can be found e.g. [here](https://en.wikipedia.org/wiki/Spectral_graph_theory).\n",
                "## A machine learning task using the neuralg module\n",
                "To carry out automatic differentation to allow, for example, gradient-based optimization, the eigenvalue approximations must not break the gradient flow. Training a neural network with PyTorch will serve as a demonstration that the `neuralg` module indeed preserves gradients. Lets say we want a neural network that approximates the total number of length $k$ cycles starting from any node in a graph. In exact arithmetic, this quantity is given by \n",
                "\n",
                "$$trace(A^k),$$\n",
                "\n",
                "where $A$ is the adjacency matrix of the graph, but we do not want to perform the $k$ matrix multiplications. From the properties of the trace operator, we can relate this quantity to  the spectrum of $A$ such that  \n",
                "\n",
                "\\begin{equation} \\# \\text{ cycles of length } k = \\sum_{i} \\lambda_i(A)^k \n",
                "\\end{equation}\n",
                "\n",
                "\n",
                "To this end, we can use `neuralg.eig()` to approximate the ground truths in the supervised learning of predicting this quantity.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import neuralg \n",
                "from neuralg.evaluation.compute_accuracy import compute_accuracy\n",
                "from neuralg.training.losses import relative_L1_evaluation_error\n",
                "import torch\n",
                "import networkx # additional requirement for processing graphs \n",
                "from copy import deepcopy\n",
                "import time\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### `neuralg` allows for setting float precision and can potentially be employed on a GPU, if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "neuralg.set_up_torch(torch_enable_cuda=True)\n",
                "neuralg.set_precision(\"float64\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Define a simple convolutional net for the regression task. A convolutional block is followed by two dense layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn \n",
                "import torch.nn.functional as F\n",
                "\n",
                "class CycleCNN(nn.Module):\n",
                "    def __init__(self, n_graph_nodes, conv_layers, filters, kernel_size):\n",
                "        super(CycleCNN, self).__init__()\n",
                "        self.net = []\n",
                "        self.n_graph_nodes = n_graph_nodes\n",
                "        self.net.append(nn.Conv2d(1,filters,kernel_size, padding = \"same\"))\n",
                "        self.net.append(nn.BatchNorm2d(filters))\n",
                "        self.net.append(nn.ReLU())\n",
                "        for i in range(conv_layers-1):\n",
                "            self.net.append(nn.Conv2d(filters,filters,kernel_size, padding = \"same\"))\n",
                "            self.net.append(nn.BatchNorm2d(filters))\n",
                "            self.net.append(nn.ReLU())\n",
                "        \n",
                "        self.net.append(nn.Conv2d(filters,1,kernel_size, padding = \"same\"))\n",
                "        self.net.append(nn.Flatten())\n",
                "        self.net.append(DenseLayer(n_graph_nodes**2,n_graph_nodes))\n",
                "        self.net.append(DenseLayer(n_graph_nodes,1, is_last = True))\n",
                "        self.net = nn.Sequential(*self.net)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        out = self.net(x)\n",
                "        return out\n",
                "\n",
                "class DenseLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, in_features, out_features, bias=True, is_last = False):\n",
                "        super().__init__()\n",
                "        self.is_last = is_last\n",
                "        self.in_features = in_features\n",
                "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
                "\n",
                "    def forward(self, input):\n",
                "        if self.is_last: \n",
                "            return self.linear(input)\n",
                "        else:\n",
                "            return F.relu(self.linear(input))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Define graph generation properties and target cycle length and necessary training parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Data parameters \n",
                "n_graph_nodes = 5 # Number of nodes in the graph, defining the size of the adjacancy matrices\n",
                "k = 5 # Cycle length of interest\n",
                "p = 0.5 # Probability of two nodes having a connecting edge \n",
                "\n",
                "\n",
                "#Training parameters\n",
                "iterations = 2000 # Iterations per epoch\n",
                "batch_size = 32 # Number of matrices per forward pass\n",
                "epochs = 5\n",
                "criterion = nn.MSELoss() # We use a mean square error loss "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_adjacency_batch(batch_size,n_graph_nodes, fixed_seed = False):\n",
                "    \"\"\" Simple generator of random graph adjacency matrices.       \n",
                "    Args:\n",
                "        batch_size (int): Size of batch to be generated\n",
                "        n_graph_nodes (int): Corresponds to size of the generaed adjacency matrices. \n",
                "        fixed_seed (bool, optional): If true, the seed is fixed in the sampling. Defaults to False.\n",
                "\n",
                "    Returns:\n",
                "        tensor: batch of adjacency matrices, of shape [batchsize,1,n_graph_nodes,n_graph_nodes] \n",
                "    \"\"\"\n",
                "    if fixed_seed:\n",
                "        torch.manual_seed(1)\n",
                "    for i in range(batch_size):\n",
                "        g = networkx.erdos_renyi_graph(n=n_graph_nodes, p= 0.5)\n",
                "        adj_matrix = torch.tensor(networkx.to_numpy_array(g))[None,:]\n",
                "        if i == 0: \n",
                "            A = adj_matrix\n",
                "        else:   \n",
                "            A = torch.cat((A,adj_matrix))\n",
                "        \n",
                "    return A[:,None,:] # broadcasts to an extra channel dimension "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model: nn.Module, use_neuralg=True) -> None:\n",
                "    model.train()  # turn on train mode\n",
                "    total_loss = 0.\n",
                "    log_interval = 1000\n",
                "    start_time = time.time()\n",
                "\n",
                "    for i in range(iterations):\n",
                "\n",
                "        A = get_adjacency_batch(batch_size, n_graph_nodes=n_graph_nodes) #Sample a batch\n",
                "\n",
                "        output = model(A) #Predict # of k-cycles with model\n",
                "\n",
                "    \n",
                "        if use_neuralg:\n",
                "            target_eigvals = neuralg.eig(A, symmetric=True) #Use neuralg module to compute ground truth\n",
                "        else:\n",
                "            target_eigvals = torch.linalg.eigvalsh(A) #Or, use torch built-in \n",
                "        \n",
                "        target = torch.pow(target_eigvals,k).sum(-1) #The target is the quantiti in Eq.(1)\n",
                "        \n",
                "        loss = criterion(output,target) # Compute loss\n",
                "    \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        loss.backward()\n",
                "        \n",
                "        optimizer.step()\n",
                "\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        if i % log_interval == 0 and i > 0:\n",
                "            lr = scheduler.get_last_lr()[0]\n",
                "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
                "            cur_loss = total_loss / log_interval\n",
                "\n",
                "            print(f'| epoch {epoch:3d} | {i:5d} batches | '\n",
                "                  f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
                "                  f'loss {cur_loss:5.5f}')\n",
                "\n",
                "            total_loss = 0\n",
                "            start_time = time.time()\n",
                "    return ms_per_batch\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training a network \n",
                "##### For reference, two identically initialized models are trained using either `neuralg.eig()` or `torch.linalg.eigvalsh()` to compute the eigenvalue ground truths. The loss is computed between the model outputs and the target function of the eigenvalues from Eq.(1), i.e. the analytical solution to number of total paths of length $k$. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trained_models = []\n",
                "for use_neuralg in [True,False]:\n",
                "    model = CycleCNN(n_graph_nodes, conv_layers = 3, filters = 32,kernel_size=3) #Instantiate model\n",
                "    lr = 3e-4 # learning rate\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
                "    model.train(); # turn on train modeepochs = 1\n",
                "    \n",
                "    for epoch in range(1, epochs + 1):\n",
                "        epoch_start_time = time.time()\n",
                "        ms_per_batch = train(model,use_neuralg)\n",
                "        scheduler.step()\n",
                "    trained_models.append([deepcopy(model),ms_per_batch])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Evaluation from training\n",
                "When evaluating on the test set, we consider a predicted output $\\hat{y}$ to be a correct solution to the problem if it approximates the ground truth $y$ to a given tolerance $\\tau$. Specifically, we check that $|y-\\hat{y}| \\leq \\tau|y|$ is satisfied. For completeness, the test set ground truths are calculated both using `neuralg.eig()` and `torch.eigvalsh()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_size = 10000 # Size of evaluation set \n",
                "eval_set = get_adjacency_batch(eval_size, n_graph_nodes, fixed_seed=True) #Sample a test set\n",
                "\n",
                "#Since path length should be an integer, we round the final predictions with the two trained models\n",
                "neuralg_pred = torch.round(trained_models[0][0](eval_set)) # Model trained with neuralg ground truths\n",
                "torch_pred = torch.round(trained_models[1][0](eval_set)) # Model trained with torch ground truths\n",
                "\n",
                "torch_targets = torch.round(torch.pow(torch.linalg.eigvalsh(eval_set),k).sum(-1)) # We evaluation eigenvalues with torch built-int\n",
                "neuralg_targets = torch.round(torch.pow(neuralg.eig(eval_set),k).sum(-1))\n",
                "\n",
                "#Model trained with neuralg \n",
                "neuralg_to_neuralg_eval_errors= relative_L1_evaluation_error(neuralg_pred,neuralg_targets)\n",
                "neuralg_to_torch_eval_errors= relative_L1_evaluation_error(neuralg_pred,torch_targets)\n",
                "\n",
                "#Model trained with torch\n",
                "torch_to_neuralg_eval_errors = relative_L1_evaluation_error(torch_pred,neuralg_targets) \n",
                "torch_to_torch_eval_errors = relative_L1_evaluation_error(torch_pred,torch_targets) \n",
                "\n",
                "tolerances = [0.1,0.08,0.06,0.05,0.025,0.02,0.01,0.005,0.0001]\n",
                "n_neuralg_acc = []\n",
                "n_torch_acc = []\n",
                "t_neuralg_acc = []\n",
                "t_torch_acc = []\n",
                "for tol in tolerances:\n",
                "    n_neuralg_acc.append(compute_accuracy(tol,neuralg_to_neuralg_eval_errors).item())\n",
                "    n_torch_acc.append(compute_accuracy(tol,neuralg_to_torch_eval_errors).item())\n",
                "    t_neuralg_acc.append(compute_accuracy(tol,torch_to_neuralg_eval_errors).item())\n",
                "    t_torch_acc.append(compute_accuracy(tol,torch_to_torch_eval_errors).item())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = plt.figure(figsize=(14, 6), dpi=150)\n",
                "fig.patch.set_facecolor(\"white\")\n",
                "ax = fig.add_subplot(1,2,1)\n",
                "ax.set_title(\"neuralg trained, {:.3} ms per batch. \\n Test accuracy versus tolerance, {}x{}\".format(trained_models[0][1],n_graph_nodes, n_graph_nodes), fontsize=18)\n",
                "ax.plot(tolerances,n_neuralg_acc,label=\"neuralg evaluation ground truths\")\n",
                "ax.plot(tolerances,n_torch_acc,label=\"torch evaluation ground truths\")\n",
                "\n",
                "ax.legend(fontsize=14)\n",
                "ax.set_xlabel(\"$\\\\tau $\", fontsize=18)\n",
                "ax.set_ylabel(\"Accuracy\", fontsize=16);\n",
                "\n",
                "ax = fig.add_subplot(1,2,2)\n",
                "ax.set_title(\"torch trained, {:.3} ms per batch.\\n Test accuracy versus tolerance, {}x{}\".format(trained_models[1][1],n_graph_nodes, n_graph_nodes), fontsize=18)\n",
                "ax.plot(tolerances,t_neuralg_acc,label=\"neuralg evaluation ground truths\")\n",
                "ax.plot(tolerances,t_torch_acc,label=\"torch evaluation ground truths\")\n",
                "\n",
                "ax.legend(fontsize=14)\n",
                "ax.set_xlabel(\"$\\\\tau $\", fontsize=18)\n",
                "ax.set_ylabel(\"Accuracy\", fontsize=16);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Conclusion \n",
                "- In evaluation, the model trained with `neuralg` eigenvalues performs well also on the more accurate torch evaluatiom ground truths. \n",
                "- Training with `neuralg` or `torch` modules seem to yield similarly performing models. \n",
                "- When run on CPU there is no synchronization needed and for these small matrices, the computational cost of the numerical methods is very small.  For larger matrices and on the GPU, we expect `neuralg` to have an edge. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### After using the module, we can clear the loaded models to free the allocated memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "neuralg.clear_loaded_models()\n",
                "assert neuralg.neuralg_ModelHandler.loaded_models == {}"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "33b13c0f5861550def7c5fd31dfce4700e1b96089b8cc03c6412b21d90f85adf"
        },
        "kernelspec": {
            "display_name": "PyCharm (pythonProject1)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
